import os
import sys
from llama_cpp import Llama

MODEL_PATH = r"/home/dmachine/Documents/RaspDbot/raspdbot-car.Q4_K_M.gguf"

SYSTEM_PROMPT = (
    "B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t cho xe t·ª± h√†nh RaspDbot-Car. "
    "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m, c√≥ th·ªÉ d√πng g·∫°ch ƒë·∫ßu d√≤ng. "
    "N·∫øu thi·∫øu d·ªØ li·ªáu th√¨ n√≥i r√µ v√† g·ª£i √Ω c·∫ßn th√¥ng tin g√¨."
)

def build_prompt(history: list[dict]) -> str:
    # Prompt ki·ªÉu chat ƒë∆°n gi·∫£n, h·ª£p v·ªõi ƒëa s·ªë model chat GGUF
    # B·∫°n c√≥ th·ªÉ thay format n·∫øu model c·ªßa b·∫°n d√πng template kh√°c.
    parts = [f"### System:\n{SYSTEM_PROMPT}\n"]
    for m in history:
        role = m["role"]
        content = m["content"]
        if role == "user":
            parts.append(f"### User:\n{content}\n")
        else:
            parts.append(f"### Assistant:\n{content}\n")
    parts.append("### Assistant:\n")
    return "\n".join(parts)

def main():
    if not os.path.exists(MODEL_PATH):
        print(f"Kh√¥ng t√¨m th·∫•y model: {MODEL_PATH}")
        sys.exit(1)

    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,      # tƒÉng/gi·∫£m t√πy RAM
        n_threads=os.cpu_count() or 4,
        n_gpu_layers=0,  # 0 = ch·∫°y CPU; n·∫øu c√≥ GPU + build CUDA th√¨ tƒÉng l√™n
        verbose=False
    )

    history: list[dict] = []
    print("ü§ñ RaspDbot-Star Chat (g√µ 'exit' ƒë·ªÉ tho√°t)\n")

    while True:
        user_text = input("B·∫°n: ").strip()
        if not user_text:
            continue
        if user_text.lower() in ("exit", "quit", "q"):
            break

        history.append({"role": "user", "content": user_text})
        prompt = build_prompt(history)

        # Sinh c√¢u tr·∫£ l·ªùi
        out = llm(
            prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.9,
            stop=["### User:", "### System:", "### Assistant:"],
        )

        answer = out["choices"][0]["text"].strip()
        if not answer:
            answer = "(Kh√¥ng sinh ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ‚Äî th·ª≠ tƒÉng max_tokens ho·∫∑c ƒë·ªïi prompt template.)"

        print(f"\nBot: {answer}\n")
        history.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()
